{"ast":null,"code":"// src/services/documentService.js\nimport * as pdfjs from 'pdfjs-dist';\npdfjs.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjs.version}/pdf.worker.min.js`;\n\n// This is a simplified in-memory store for document content\n// In a real application, you'd use a database or vector store\nlet documentContent = [];\nexport const processDocument = async file => {\n  try {\n    console.log(\"Processing document:\", file.name);\n\n    // Reset document content\n    documentContent = [];\n\n    // Read the file as ArrayBuffer\n    const arrayBuffer = await file.arrayBuffer();\n\n    // Load the PDF document\n    const pdf = await pdfjs.getDocument({\n      data: arrayBuffer\n    }).promise;\n    const numPages = pdf.numPages;\n    console.log(`PDF has ${numPages} pages`);\n\n    // Process each page\n    for (let i = 1; i <= numPages; i++) {\n      const page = await pdf.getPage(i);\n      const content = await page.getTextContent();\n      const text = content.items.map(item => item.str).join(' ');\n\n      // Split text into chunks (simplified approach)\n      const chunks = splitIntoChunks(text, 1000); // Split into chunks of ~1000 chars\n\n      // Store chunks - Fixed: Don't define a function inside a loop\n      storeChunks(chunks, i, file.name);\n    }\n    console.log(`Extracted ${documentContent.length} chunks from the document`);\n    return documentContent.length;\n  } catch (error) {\n    console.error(\"Error processing document:\", error);\n    throw error;\n  }\n};\n\n// Store chunks in the document content array\nconst storeChunks = (chunks, pageNumber, source) => {\n  chunks.forEach(chunk => {\n    documentContent.push({\n      text: chunk,\n      page: pageNumber,\n      source: source\n    });\n  });\n};\n\n// Split text into chunks of approximately the specified size\nconst splitIntoChunks = (text, chunkSize) => {\n  const chunks = [];\n  let startIndex = 0;\n  while (startIndex < text.length) {\n    let endIndex = Math.min(startIndex + chunkSize, text.length);\n\n    // Try to find a natural break point\n    if (endIndex < text.length) {\n      const breakPoints = ['. ', '! ', '? ', '\\n\\n', '\\n'];\n      for (const breakPoint of breakPoints) {\n        const lastBreakIndex = text.lastIndexOf(breakPoint, endIndex);\n        if (lastBreakIndex > startIndex && lastBreakIndex < endIndex) {\n          endIndex = lastBreakIndex + breakPoint.length;\n          break;\n        }\n      }\n    }\n    chunks.push(text.substring(startIndex, endIndex).trim());\n    startIndex = endIndex;\n  }\n  return chunks;\n};\n\n// Utility function to search document content (for use with AI queries)\nexport const searchDocumentContent = query => {\n  // Simple keyword search (in a real app, use semantic search)\n  const keywords = query.toLowerCase().split(' ');\n  return documentContent.filter(chunk => keywords.some(keyword => chunk.text.toLowerCase().includes(keyword)));\n};","map":{"version":3,"names":["pdfjs","GlobalWorkerOptions","workerSrc","version","documentContent","processDocument","file","console","log","name","arrayBuffer","pdf","getDocument","data","promise","numPages","i","page","getPage","content","getTextContent","text","items","map","item","str","join","chunks","splitIntoChunks","storeChunks","length","error","pageNumber","source","forEach","chunk","push","chunkSize","startIndex","endIndex","Math","min","breakPoints","breakPoint","lastBreakIndex","lastIndexOf","substring","trim","searchDocumentContent","query","keywords","toLowerCase","split","filter","some","keyword","includes"],"sources":["/Users/madhu.sudhan/Documents/logistics-chatbot/src/services/documentService.js"],"sourcesContent":["// src/services/documentService.js\nimport * as pdfjs from 'pdfjs-dist';\npdfjs.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjs.version}/pdf.worker.min.js`;\n\n// This is a simplified in-memory store for document content\n// In a real application, you'd use a database or vector store\nlet documentContent = [];\n\nexport const processDocument = async (file) => {\n  try {\n    console.log(\"Processing document:\", file.name);\n    \n    // Reset document content\n    documentContent = [];\n    \n    // Read the file as ArrayBuffer\n    const arrayBuffer = await file.arrayBuffer();\n    \n    // Load the PDF document\n    const pdf = await pdfjs.getDocument({ data: arrayBuffer }).promise;\n    const numPages = pdf.numPages;\n    \n    console.log(`PDF has ${numPages} pages`);\n    \n    // Process each page\n    for (let i = 1; i <= numPages; i++) {\n      const page = await pdf.getPage(i);\n      const content = await page.getTextContent();\n      const text = content.items.map(item => item.str).join(' ');\n      \n      // Split text into chunks (simplified approach)\n      const chunks = splitIntoChunks(text, 1000); // Split into chunks of ~1000 chars\n      \n      // Store chunks - Fixed: Don't define a function inside a loop\n      storeChunks(chunks, i, file.name);\n    }\n    \n    console.log(`Extracted ${documentContent.length} chunks from the document`);\n    return documentContent.length;\n  } catch (error) {\n    console.error(\"Error processing document:\", error);\n    throw error;\n  }\n};\n\n// Store chunks in the document content array\nconst storeChunks = (chunks, pageNumber, source) => {\n  chunks.forEach(chunk => {\n    documentContent.push({\n      text: chunk,\n      page: pageNumber,\n      source: source\n    });\n  });\n};\n\n// Split text into chunks of approximately the specified size\nconst splitIntoChunks = (text, chunkSize) => {\n  const chunks = [];\n  let startIndex = 0;\n  \n  while (startIndex < text.length) {\n    let endIndex = Math.min(startIndex + chunkSize, text.length);\n    \n    // Try to find a natural break point\n    if (endIndex < text.length) {\n      const breakPoints = ['. ', '! ', '? ', '\\n\\n', '\\n'];\n      \n      for (const breakPoint of breakPoints) {\n        const lastBreakIndex = text.lastIndexOf(breakPoint, endIndex);\n        if (lastBreakIndex > startIndex && lastBreakIndex < endIndex) {\n          endIndex = lastBreakIndex + breakPoint.length;\n          break;\n        }\n      }\n    }\n    \n    chunks.push(text.substring(startIndex, endIndex).trim());\n    startIndex = endIndex;\n  }\n  \n  return chunks;\n};\n\n// Utility function to search document content (for use with AI queries)\nexport const searchDocumentContent = (query) => {\n  // Simple keyword search (in a real app, use semantic search)\n  const keywords = query.toLowerCase().split(' ');\n  \n  return documentContent.filter(chunk => \n    keywords.some(keyword => \n      chunk.text.toLowerCase().includes(keyword)\n    )\n  );\n};\n"],"mappings":"AAAA;AACA,OAAO,KAAKA,KAAK,MAAM,YAAY;AACnCA,KAAK,CAACC,mBAAmB,CAACC,SAAS,GAAG,2CAA2CF,KAAK,CAACG,OAAO,oBAAoB;;AAElH;AACA;AACA,IAAIC,eAAe,GAAG,EAAE;AAExB,OAAO,MAAMC,eAAe,GAAG,MAAOC,IAAI,IAAK;EAC7C,IAAI;IACFC,OAAO,CAACC,GAAG,CAAC,sBAAsB,EAAEF,IAAI,CAACG,IAAI,CAAC;;IAE9C;IACAL,eAAe,GAAG,EAAE;;IAEpB;IACA,MAAMM,WAAW,GAAG,MAAMJ,IAAI,CAACI,WAAW,CAAC,CAAC;;IAE5C;IACA,MAAMC,GAAG,GAAG,MAAMX,KAAK,CAACY,WAAW,CAAC;MAAEC,IAAI,EAAEH;IAAY,CAAC,CAAC,CAACI,OAAO;IAClE,MAAMC,QAAQ,GAAGJ,GAAG,CAACI,QAAQ;IAE7BR,OAAO,CAACC,GAAG,CAAC,WAAWO,QAAQ,QAAQ,CAAC;;IAExC;IACA,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,IAAID,QAAQ,EAAEC,CAAC,EAAE,EAAE;MAClC,MAAMC,IAAI,GAAG,MAAMN,GAAG,CAACO,OAAO,CAACF,CAAC,CAAC;MACjC,MAAMG,OAAO,GAAG,MAAMF,IAAI,CAACG,cAAc,CAAC,CAAC;MAC3C,MAAMC,IAAI,GAAGF,OAAO,CAACG,KAAK,CAACC,GAAG,CAACC,IAAI,IAAIA,IAAI,CAACC,GAAG,CAAC,CAACC,IAAI,CAAC,GAAG,CAAC;;MAE1D;MACA,MAAMC,MAAM,GAAGC,eAAe,CAACP,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;;MAE5C;MACAQ,WAAW,CAACF,MAAM,EAAEX,CAAC,EAAEV,IAAI,CAACG,IAAI,CAAC;IACnC;IAEAF,OAAO,CAACC,GAAG,CAAC,aAAaJ,eAAe,CAAC0B,MAAM,2BAA2B,CAAC;IAC3E,OAAO1B,eAAe,CAAC0B,MAAM;EAC/B,CAAC,CAAC,OAAOC,KAAK,EAAE;IACdxB,OAAO,CAACwB,KAAK,CAAC,4BAA4B,EAAEA,KAAK,CAAC;IAClD,MAAMA,KAAK;EACb;AACF,CAAC;;AAED;AACA,MAAMF,WAAW,GAAGA,CAACF,MAAM,EAAEK,UAAU,EAAEC,MAAM,KAAK;EAClDN,MAAM,CAACO,OAAO,CAACC,KAAK,IAAI;IACtB/B,eAAe,CAACgC,IAAI,CAAC;MACnBf,IAAI,EAAEc,KAAK;MACXlB,IAAI,EAAEe,UAAU;MAChBC,MAAM,EAAEA;IACV,CAAC,CAAC;EACJ,CAAC,CAAC;AACJ,CAAC;;AAED;AACA,MAAML,eAAe,GAAGA,CAACP,IAAI,EAAEgB,SAAS,KAAK;EAC3C,MAAMV,MAAM,GAAG,EAAE;EACjB,IAAIW,UAAU,GAAG,CAAC;EAElB,OAAOA,UAAU,GAAGjB,IAAI,CAACS,MAAM,EAAE;IAC/B,IAAIS,QAAQ,GAAGC,IAAI,CAACC,GAAG,CAACH,UAAU,GAAGD,SAAS,EAAEhB,IAAI,CAACS,MAAM,CAAC;;IAE5D;IACA,IAAIS,QAAQ,GAAGlB,IAAI,CAACS,MAAM,EAAE;MAC1B,MAAMY,WAAW,GAAG,CAAC,IAAI,EAAE,IAAI,EAAE,IAAI,EAAE,MAAM,EAAE,IAAI,CAAC;MAEpD,KAAK,MAAMC,UAAU,IAAID,WAAW,EAAE;QACpC,MAAME,cAAc,GAAGvB,IAAI,CAACwB,WAAW,CAACF,UAAU,EAAEJ,QAAQ,CAAC;QAC7D,IAAIK,cAAc,GAAGN,UAAU,IAAIM,cAAc,GAAGL,QAAQ,EAAE;UAC5DA,QAAQ,GAAGK,cAAc,GAAGD,UAAU,CAACb,MAAM;UAC7C;QACF;MACF;IACF;IAEAH,MAAM,CAACS,IAAI,CAACf,IAAI,CAACyB,SAAS,CAACR,UAAU,EAAEC,QAAQ,CAAC,CAACQ,IAAI,CAAC,CAAC,CAAC;IACxDT,UAAU,GAAGC,QAAQ;EACvB;EAEA,OAAOZ,MAAM;AACf,CAAC;;AAED;AACA,OAAO,MAAMqB,qBAAqB,GAAIC,KAAK,IAAK;EAC9C;EACA,MAAMC,QAAQ,GAAGD,KAAK,CAACE,WAAW,CAAC,CAAC,CAACC,KAAK,CAAC,GAAG,CAAC;EAE/C,OAAOhD,eAAe,CAACiD,MAAM,CAAClB,KAAK,IACjCe,QAAQ,CAACI,IAAI,CAACC,OAAO,IACnBpB,KAAK,CAACd,IAAI,CAAC8B,WAAW,CAAC,CAAC,CAACK,QAAQ,CAACD,OAAO,CAC3C,CACF,CAAC;AACH,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}