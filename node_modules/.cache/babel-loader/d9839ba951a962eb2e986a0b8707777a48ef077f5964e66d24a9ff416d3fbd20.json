{"ast":null,"code":"// src/services/aiService.js\nlet documentContext = [];\nexport const setDocumentContext = context => {\n  documentContext = context;\n  console.log(\"Document context updated:\", documentContext.length, \"chunks\");\n};\nexport const testGeminiAPIConnection = async () => {\n  try {\n    const API_KEY = process.env.REACT_APP_GEMINI_API_KEY;\n    if (!API_KEY) {\n      return {\n        success: false,\n        message: \"API key not configured\"\n      };\n    }\n\n    // Use the correct endpoint with the latest model name\n    const testEndpoint = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\";\n    const testMessage = \"Hello, this is a test message.\";\n    const response = await fetch(`${testEndpoint}?key=${API_KEY}`, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\"\n      },\n      body: JSON.stringify({\n        contents: [{\n          parts: [{\n            text: testMessage\n          }]\n        }]\n      })\n    });\n    const statusCode = response.status;\n    let responseText = \"\";\n    try {\n      const responseData = await response.json();\n      responseText = JSON.stringify(responseData, null, 2);\n    } catch (e) {\n      responseText = await response.text();\n    }\n    return {\n      success: response.ok,\n      statusCode,\n      responseText,\n      message: response.ok ? \"API connection successful\" : `API error: ${statusCode}`\n    };\n  } catch (error) {\n    return {\n      success: false,\n      message: `Connection error: ${error.message}`,\n      error\n    };\n  }\n};\nexport const queryGeminiAI = async message => {\n  try {\n    const API_KEY = process.env.REACT_APP_GEMINI_API_KEY;\n    if (!API_KEY) {\n      console.error(\"Missing Gemini API key. Set REACT_APP_GEMINI_API_KEY in your environment.\");\n      throw new Error(\"API key not configured\");\n    }\n    console.log(\"Using API key starting with:\", API_KEY.substring(0, 3) + \"...\" + API_KEY.slice(-3));\n\n    // Prepare the full prompt\n    let fullPrompt = \"You are an AI assistant specializing in last mile logistics in India. \";\n    fullPrompt += \"Provide detailed, accurate information about delivery services, tracking, shipping costs, and logistics coverage areas in India. \";\n    fullPrompt += \"Be helpful, concise, and focus on Indian logistics context.\\n\\n\";\n    fullPrompt += \"User question: \" + message;\n\n    // Add document context if available\n    if (documentContext && documentContext.length > 0) {\n      fullPrompt += \"\\n\\nReference these document sections when applicable:\\n\";\n      const relevantChunks = documentContext.slice(0, 3);\n      relevantChunks.forEach((chunk, i) => {\n        fullPrompt += `\\nSection ${i + 1}: ${chunk}\\n`;\n      });\n      fullPrompt += \"\\nPlease use this document information when relevant to answer the user's question.\";\n    }\n\n    // Use the correct endpoint and model name\n    // Try gemini-1.5-flash first (current model)\n    const apiUrl = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\";\n\n    // Format the request \n    const requestBody = {\n      contents: [{\n        parts: [{\n          text: fullPrompt\n        }]\n      }],\n      generationConfig: {\n        temperature: 0.7,\n        maxOutputTokens: 800\n      }\n    };\n    console.log(\"Sending request to Gemini API at:\", apiUrl);\n\n    // Send the request\n    const response = await fetch(`${apiUrl}?key=${API_KEY}`, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\"\n      },\n      body: JSON.stringify(requestBody)\n    });\n    console.log(\"API Response status:\", response.status);\n    if (!response.ok) {\n      const errorText = await response.text();\n      console.error(\"API Error:\", response.status, errorText);\n      if (response.status === 404) {\n        // Try fallback to gemini-1.0-pro if 1.5-flash isn't found\n        console.log(\"Model not found, trying fallback to gemini-1.0-pro\");\n        return await queryWithFallbackModel(fullPrompt, API_KEY);\n      } else if (response.status === 403) {\n        throw new Error(\"API access forbidden. Check if API key has correct permissions.\");\n      } else if (response.status === 429) {\n        throw new Error(\"Rate limit exceeded. Please try again later.\");\n      } else {\n        throw new Error(`API error: ${response.status} ${errorText}`);\n      }\n    }\n    const data = await response.json();\n\n    // Extract text from the response\n    let text = \"\";\n    if (data.candidates && data.candidates.length > 0 && data.candidates[0].content && data.candidates[0].content.parts) {\n      text = data.candidates[0].content.parts.map(part => part.text || \"\").join(\"\");\n      console.log(\"Successfully extracted text, length:\", text.length);\n    } else {\n      console.warn(\"Unexpected API response structure:\", JSON.stringify(data, null, 2));\n      throw new Error(\"Unexpected API response structure\");\n    }\n    return text;\n  } catch (error) {\n    console.error(\"Error in Gemini API call:\", error);\n    throw error;\n  }\n};\n\n// Fallback function to try different models\nasync function queryWithFallbackModel(prompt, apiKey) {\n  // Try a sequence of models until one works\n  const models = [\"gemini-1.0-pro\", \"gemini-pro\", \"gemini-pro-latest\"];\n  let lastError = null;\n  for (const model of models) {\n    try {\n      console.log(`Trying model: ${model}`);\n      const fallbackUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent`;\n      const requestBody = {\n        contents: [{\n          parts: [{\n            text: prompt\n          }]\n        }],\n        generationConfig: {\n          temperature: 0.7,\n          maxOutputTokens: 800\n        }\n      };\n      const response = await fetch(`${fallbackUrl}?key=${apiKey}`, {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\"\n        },\n        body: JSON.stringify(requestBody)\n      });\n      if (!response.ok) {\n        const errorText = await response.text();\n        console.log(`Model ${model} failed with status:`, response.status, errorText);\n        lastError = new Error(`API error with ${model}: ${response.status}`);\n        continue; // Try next model\n      }\n      const data = await response.json();\n      if (data.candidates && data.candidates.length > 0 && data.candidates[0].content && data.candidates[0].content.parts) {\n        const text = data.candidates[0].content.parts.map(part => part.text || \"\").join(\"\");\n        console.log(`Successfully used fallback model: ${model}`);\n        return text;\n      } else {\n        console.warn(`Unexpected response from ${model}:`, JSON.stringify(data, null, 2));\n        lastError = new Error(`Unexpected response structure from ${model}`);\n        continue; // Try next model\n      }\n    } catch (error) {\n      console.error(`Error with model ${model}:`, error);\n      lastError = error;\n      // Continue to the next model\n    }\n  }\n\n  // If we get here, all models failed\n  throw lastError || new Error(\"All fallback models failed\");\n}","map":{"version":3,"names":["documentContext","setDocumentContext","context","console","log","length","testGeminiAPIConnection","API_KEY","process","env","REACT_APP_GEMINI_API_KEY","success","message","testEndpoint","testMessage","response","fetch","method","headers","body","JSON","stringify","contents","parts","text","statusCode","status","responseText","responseData","json","e","ok","error","queryGeminiAI","Error","substring","slice","fullPrompt","relevantChunks","forEach","chunk","i","apiUrl","requestBody","generationConfig","temperature","maxOutputTokens","errorText","queryWithFallbackModel","data","candidates","content","map","part","join","warn","prompt","apiKey","models","lastError","model","fallbackUrl"],"sources":["/Users/madhu.sudhan/Documents/logistics-chatbot/src/services/aiService.js"],"sourcesContent":["// src/services/aiService.js\nlet documentContext = [];\n\nexport const setDocumentContext = (context) => {\n  documentContext = context;\n  console.log(\"Document context updated:\", documentContext.length, \"chunks\");\n};\n\nexport const testGeminiAPIConnection = async () => {\n  try {\n    const API_KEY = process.env.REACT_APP_GEMINI_API_KEY;\n    \n    if (!API_KEY) {\n      return { success: false, message: \"API key not configured\" };\n    }\n    \n    // Use the correct endpoint with the latest model name\n    const testEndpoint = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\";\n    const testMessage = \"Hello, this is a test message.\";\n    \n    const response = await fetch(\n      `${testEndpoint}?key=${API_KEY}`,\n      {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\"\n        },\n        body: JSON.stringify({\n          contents: [{ parts: [{ text: testMessage }] }]\n        })\n      }\n    );\n    \n    const statusCode = response.status;\n    let responseText = \"\";\n    \n    try {\n      const responseData = await response.json();\n      responseText = JSON.stringify(responseData, null, 2);\n    } catch (e) {\n      responseText = await response.text();\n    }\n    \n    return {\n      success: response.ok,\n      statusCode,\n      responseText,\n      message: response.ok ? \"API connection successful\" : `API error: ${statusCode}`\n    };\n  } catch (error) {\n    return {\n      success: false,\n      message: `Connection error: ${error.message}`,\n      error\n    };\n  }\n};\n\nexport const queryGeminiAI = async (message) => {\n  try {\n    const API_KEY = process.env.REACT_APP_GEMINI_API_KEY;\n    \n    if (!API_KEY) {\n      console.error(\"Missing Gemini API key. Set REACT_APP_GEMINI_API_KEY in your environment.\");\n      throw new Error(\"API key not configured\");\n    }\n\n    console.log(\"Using API key starting with:\", API_KEY.substring(0, 3) + \"...\" + API_KEY.slice(-3));\n\n    // Prepare the full prompt\n    let fullPrompt = \"You are an AI assistant specializing in last mile logistics in India. \";\n    fullPrompt += \"Provide detailed, accurate information about delivery services, tracking, shipping costs, and logistics coverage areas in India. \";\n    fullPrompt += \"Be helpful, concise, and focus on Indian logistics context.\\n\\n\";\n    fullPrompt += \"User question: \" + message;\n    \n    // Add document context if available\n    if (documentContext && documentContext.length > 0) {\n      fullPrompt += \"\\n\\nReference these document sections when applicable:\\n\";\n      const relevantChunks = documentContext.slice(0, 3);\n      relevantChunks.forEach((chunk, i) => {\n        fullPrompt += `\\nSection ${i+1}: ${chunk}\\n`;\n      });\n      fullPrompt += \"\\nPlease use this document information when relevant to answer the user's question.\";\n    }\n\n    // Use the correct endpoint and model name\n    // Try gemini-1.5-flash first (current model)\n    const apiUrl = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\";\n    \n    // Format the request \n    const requestBody = {\n      contents: [\n        {\n          parts: [\n            { text: fullPrompt }\n          ]\n        }\n      ],\n      generationConfig: {\n        temperature: 0.7,\n        maxOutputTokens: 800\n      }\n    };\n    \n    console.log(\"Sending request to Gemini API at:\", apiUrl);\n    \n    // Send the request\n    const response = await fetch(\n      `${apiUrl}?key=${API_KEY}`,\n      {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\"\n        },\n        body: JSON.stringify(requestBody)\n      }\n    );\n\n    console.log(\"API Response status:\", response.status);\n    \n    if (!response.ok) {\n      const errorText = await response.text();\n      console.error(\"API Error:\", response.status, errorText);\n      \n      if (response.status === 404) {\n        // Try fallback to gemini-1.0-pro if 1.5-flash isn't found\n        console.log(\"Model not found, trying fallback to gemini-1.0-pro\");\n        return await queryWithFallbackModel(fullPrompt, API_KEY);\n      } else if (response.status === 403) {\n        throw new Error(\"API access forbidden. Check if API key has correct permissions.\");\n      } else if (response.status === 429) {\n        throw new Error(\"Rate limit exceeded. Please try again later.\");\n      } else {\n        throw new Error(`API error: ${response.status} ${errorText}`);\n      }\n    }\n\n    const data = await response.json();\n    \n    // Extract text from the response\n    let text = \"\";\n    if (data.candidates && \n        data.candidates.length > 0 && \n        data.candidates[0].content && \n        data.candidates[0].content.parts) {\n      text = data.candidates[0].content.parts\n        .map(part => part.text || \"\")\n        .join(\"\");\n      console.log(\"Successfully extracted text, length:\", text.length);\n    } else {\n      console.warn(\"Unexpected API response structure:\", JSON.stringify(data, null, 2));\n      throw new Error(\"Unexpected API response structure\");\n    }\n    \n    return text;\n  } catch (error) {\n    console.error(\"Error in Gemini API call:\", error);\n    throw error;\n  }\n};\n\n// Fallback function to try different models\nasync function queryWithFallbackModel(prompt, apiKey) {\n  // Try a sequence of models until one works\n  const models = [\n    \"gemini-1.0-pro\",\n    \"gemini-pro\",\n    \"gemini-pro-latest\"\n  ];\n  \n  let lastError = null;\n  \n  for (const model of models) {\n    try {\n      console.log(`Trying model: ${model}`);\n      const fallbackUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent`;\n      \n      const requestBody = {\n        contents: [\n          {\n            parts: [\n              { text: prompt }\n            ]\n          }\n        ],\n        generationConfig: {\n          temperature: 0.7,\n          maxOutputTokens: 800\n        }\n      };\n      \n      const response = await fetch(\n        `${fallbackUrl}?key=${apiKey}`,\n        {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\"\n          },\n          body: JSON.stringify(requestBody)\n        }\n      );\n      \n      if (!response.ok) {\n        const errorText = await response.text();\n        console.log(`Model ${model} failed with status:`, response.status, errorText);\n        lastError = new Error(`API error with ${model}: ${response.status}`);\n        continue; // Try next model\n      }\n      \n      const data = await response.json();\n      \n      if (data.candidates && \n          data.candidates.length > 0 && \n          data.candidates[0].content && \n          data.candidates[0].content.parts) {\n        const text = data.candidates[0].content.parts\n          .map(part => part.text || \"\")\n          .join(\"\");\n        console.log(`Successfully used fallback model: ${model}`);\n        return text;\n      } else {\n        console.warn(`Unexpected response from ${model}:`, JSON.stringify(data, null, 2));\n        lastError = new Error(`Unexpected response structure from ${model}`);\n        continue; // Try next model\n      }\n    } catch (error) {\n      console.error(`Error with model ${model}:`, error);\n      lastError = error;\n      // Continue to the next model\n    }\n  }\n  \n  // If we get here, all models failed\n  throw lastError || new Error(\"All fallback models failed\");\n}\n"],"mappings":"AAAA;AACA,IAAIA,eAAe,GAAG,EAAE;AAExB,OAAO,MAAMC,kBAAkB,GAAIC,OAAO,IAAK;EAC7CF,eAAe,GAAGE,OAAO;EACzBC,OAAO,CAACC,GAAG,CAAC,2BAA2B,EAAEJ,eAAe,CAACK,MAAM,EAAE,QAAQ,CAAC;AAC5E,CAAC;AAED,OAAO,MAAMC,uBAAuB,GAAG,MAAAA,CAAA,KAAY;EACjD,IAAI;IACF,MAAMC,OAAO,GAAGC,OAAO,CAACC,GAAG,CAACC,wBAAwB;IAEpD,IAAI,CAACH,OAAO,EAAE;MACZ,OAAO;QAAEI,OAAO,EAAE,KAAK;QAAEC,OAAO,EAAE;MAAyB,CAAC;IAC9D;;IAEA;IACA,MAAMC,YAAY,GAAG,0FAA0F;IAC/G,MAAMC,WAAW,GAAG,gCAAgC;IAEpD,MAAMC,QAAQ,GAAG,MAAMC,KAAK,CAC1B,GAAGH,YAAY,QAAQN,OAAO,EAAE,EAChC;MACEU,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;QACnBC,QAAQ,EAAE,CAAC;UAAEC,KAAK,EAAE,CAAC;YAAEC,IAAI,EAAEV;UAAY,CAAC;QAAE,CAAC;MAC/C,CAAC;IACH,CACF,CAAC;IAED,MAAMW,UAAU,GAAGV,QAAQ,CAACW,MAAM;IAClC,IAAIC,YAAY,GAAG,EAAE;IAErB,IAAI;MACF,MAAMC,YAAY,GAAG,MAAMb,QAAQ,CAACc,IAAI,CAAC,CAAC;MAC1CF,YAAY,GAAGP,IAAI,CAACC,SAAS,CAACO,YAAY,EAAE,IAAI,EAAE,CAAC,CAAC;IACtD,CAAC,CAAC,OAAOE,CAAC,EAAE;MACVH,YAAY,GAAG,MAAMZ,QAAQ,CAACS,IAAI,CAAC,CAAC;IACtC;IAEA,OAAO;MACLb,OAAO,EAAEI,QAAQ,CAACgB,EAAE;MACpBN,UAAU;MACVE,YAAY;MACZf,OAAO,EAAEG,QAAQ,CAACgB,EAAE,GAAG,2BAA2B,GAAG,cAAcN,UAAU;IAC/E,CAAC;EACH,CAAC,CAAC,OAAOO,KAAK,EAAE;IACd,OAAO;MACLrB,OAAO,EAAE,KAAK;MACdC,OAAO,EAAE,qBAAqBoB,KAAK,CAACpB,OAAO,EAAE;MAC7CoB;IACF,CAAC;EACH;AACF,CAAC;AAED,OAAO,MAAMC,aAAa,GAAG,MAAOrB,OAAO,IAAK;EAC9C,IAAI;IACF,MAAML,OAAO,GAAGC,OAAO,CAACC,GAAG,CAACC,wBAAwB;IAEpD,IAAI,CAACH,OAAO,EAAE;MACZJ,OAAO,CAAC6B,KAAK,CAAC,2EAA2E,CAAC;MAC1F,MAAM,IAAIE,KAAK,CAAC,wBAAwB,CAAC;IAC3C;IAEA/B,OAAO,CAACC,GAAG,CAAC,8BAA8B,EAAEG,OAAO,CAAC4B,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,KAAK,GAAG5B,OAAO,CAAC6B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;;IAEhG;IACA,IAAIC,UAAU,GAAG,wEAAwE;IACzFA,UAAU,IAAI,mIAAmI;IACjJA,UAAU,IAAI,iEAAiE;IAC/EA,UAAU,IAAI,iBAAiB,GAAGzB,OAAO;;IAEzC;IACA,IAAIZ,eAAe,IAAIA,eAAe,CAACK,MAAM,GAAG,CAAC,EAAE;MACjDgC,UAAU,IAAI,0DAA0D;MACxE,MAAMC,cAAc,GAAGtC,eAAe,CAACoC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC;MAClDE,cAAc,CAACC,OAAO,CAAC,CAACC,KAAK,EAAEC,CAAC,KAAK;QACnCJ,UAAU,IAAI,aAAaI,CAAC,GAAC,CAAC,KAAKD,KAAK,IAAI;MAC9C,CAAC,CAAC;MACFH,UAAU,IAAI,qFAAqF;IACrG;;IAEA;IACA;IACA,MAAMK,MAAM,GAAG,0FAA0F;;IAEzG;IACA,MAAMC,WAAW,GAAG;MAClBrB,QAAQ,EAAE,CACR;QACEC,KAAK,EAAE,CACL;UAAEC,IAAI,EAAEa;QAAW,CAAC;MAExB,CAAC,CACF;MACDO,gBAAgB,EAAE;QAChBC,WAAW,EAAE,GAAG;QAChBC,eAAe,EAAE;MACnB;IACF,CAAC;IAED3C,OAAO,CAACC,GAAG,CAAC,mCAAmC,EAAEsC,MAAM,CAAC;;IAExD;IACA,MAAM3B,QAAQ,GAAG,MAAMC,KAAK,CAC1B,GAAG0B,MAAM,QAAQnC,OAAO,EAAE,EAC1B;MACEU,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACsB,WAAW;IAClC,CACF,CAAC;IAEDxC,OAAO,CAACC,GAAG,CAAC,sBAAsB,EAAEW,QAAQ,CAACW,MAAM,CAAC;IAEpD,IAAI,CAACX,QAAQ,CAACgB,EAAE,EAAE;MAChB,MAAMgB,SAAS,GAAG,MAAMhC,QAAQ,CAACS,IAAI,CAAC,CAAC;MACvCrB,OAAO,CAAC6B,KAAK,CAAC,YAAY,EAAEjB,QAAQ,CAACW,MAAM,EAAEqB,SAAS,CAAC;MAEvD,IAAIhC,QAAQ,CAACW,MAAM,KAAK,GAAG,EAAE;QAC3B;QACAvB,OAAO,CAACC,GAAG,CAAC,oDAAoD,CAAC;QACjE,OAAO,MAAM4C,sBAAsB,CAACX,UAAU,EAAE9B,OAAO,CAAC;MAC1D,CAAC,MAAM,IAAIQ,QAAQ,CAACW,MAAM,KAAK,GAAG,EAAE;QAClC,MAAM,IAAIQ,KAAK,CAAC,iEAAiE,CAAC;MACpF,CAAC,MAAM,IAAInB,QAAQ,CAACW,MAAM,KAAK,GAAG,EAAE;QAClC,MAAM,IAAIQ,KAAK,CAAC,8CAA8C,CAAC;MACjE,CAAC,MAAM;QACL,MAAM,IAAIA,KAAK,CAAC,cAAcnB,QAAQ,CAACW,MAAM,IAAIqB,SAAS,EAAE,CAAC;MAC/D;IACF;IAEA,MAAME,IAAI,GAAG,MAAMlC,QAAQ,CAACc,IAAI,CAAC,CAAC;;IAElC;IACA,IAAIL,IAAI,GAAG,EAAE;IACb,IAAIyB,IAAI,CAACC,UAAU,IACfD,IAAI,CAACC,UAAU,CAAC7C,MAAM,GAAG,CAAC,IAC1B4C,IAAI,CAACC,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,IAC1BF,IAAI,CAACC,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC5B,KAAK,EAAE;MACpCC,IAAI,GAAGyB,IAAI,CAACC,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC5B,KAAK,CACpC6B,GAAG,CAACC,IAAI,IAAIA,IAAI,CAAC7B,IAAI,IAAI,EAAE,CAAC,CAC5B8B,IAAI,CAAC,EAAE,CAAC;MACXnD,OAAO,CAACC,GAAG,CAAC,sCAAsC,EAAEoB,IAAI,CAACnB,MAAM,CAAC;IAClE,CAAC,MAAM;MACLF,OAAO,CAACoD,IAAI,CAAC,oCAAoC,EAAEnC,IAAI,CAACC,SAAS,CAAC4B,IAAI,EAAE,IAAI,EAAE,CAAC,CAAC,CAAC;MACjF,MAAM,IAAIf,KAAK,CAAC,mCAAmC,CAAC;IACtD;IAEA,OAAOV,IAAI;EACb,CAAC,CAAC,OAAOQ,KAAK,EAAE;IACd7B,OAAO,CAAC6B,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;IACjD,MAAMA,KAAK;EACb;AACF,CAAC;;AAED;AACA,eAAegB,sBAAsBA,CAACQ,MAAM,EAAEC,MAAM,EAAE;EACpD;EACA,MAAMC,MAAM,GAAG,CACb,gBAAgB,EAChB,YAAY,EACZ,mBAAmB,CACpB;EAED,IAAIC,SAAS,GAAG,IAAI;EAEpB,KAAK,MAAMC,KAAK,IAAIF,MAAM,EAAE;IAC1B,IAAI;MACFvD,OAAO,CAACC,GAAG,CAAC,iBAAiBwD,KAAK,EAAE,CAAC;MACrC,MAAMC,WAAW,GAAG,2DAA2DD,KAAK,kBAAkB;MAEtG,MAAMjB,WAAW,GAAG;QAClBrB,QAAQ,EAAE,CACR;UACEC,KAAK,EAAE,CACL;YAAEC,IAAI,EAAEgC;UAAO,CAAC;QAEpB,CAAC,CACF;QACDZ,gBAAgB,EAAE;UAChBC,WAAW,EAAE,GAAG;UAChBC,eAAe,EAAE;QACnB;MACF,CAAC;MAED,MAAM/B,QAAQ,GAAG,MAAMC,KAAK,CAC1B,GAAG6C,WAAW,QAAQJ,MAAM,EAAE,EAC9B;QACExC,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACP,cAAc,EAAE;QAClB,CAAC;QACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACsB,WAAW;MAClC,CACF,CAAC;MAED,IAAI,CAAC5B,QAAQ,CAACgB,EAAE,EAAE;QAChB,MAAMgB,SAAS,GAAG,MAAMhC,QAAQ,CAACS,IAAI,CAAC,CAAC;QACvCrB,OAAO,CAACC,GAAG,CAAC,SAASwD,KAAK,sBAAsB,EAAE7C,QAAQ,CAACW,MAAM,EAAEqB,SAAS,CAAC;QAC7EY,SAAS,GAAG,IAAIzB,KAAK,CAAC,kBAAkB0B,KAAK,KAAK7C,QAAQ,CAACW,MAAM,EAAE,CAAC;QACpE,SAAS,CAAC;MACZ;MAEA,MAAMuB,IAAI,GAAG,MAAMlC,QAAQ,CAACc,IAAI,CAAC,CAAC;MAElC,IAAIoB,IAAI,CAACC,UAAU,IACfD,IAAI,CAACC,UAAU,CAAC7C,MAAM,GAAG,CAAC,IAC1B4C,IAAI,CAACC,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,IAC1BF,IAAI,CAACC,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC5B,KAAK,EAAE;QACpC,MAAMC,IAAI,GAAGyB,IAAI,CAACC,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC5B,KAAK,CAC1C6B,GAAG,CAACC,IAAI,IAAIA,IAAI,CAAC7B,IAAI,IAAI,EAAE,CAAC,CAC5B8B,IAAI,CAAC,EAAE,CAAC;QACXnD,OAAO,CAACC,GAAG,CAAC,qCAAqCwD,KAAK,EAAE,CAAC;QACzD,OAAOpC,IAAI;MACb,CAAC,MAAM;QACLrB,OAAO,CAACoD,IAAI,CAAC,4BAA4BK,KAAK,GAAG,EAAExC,IAAI,CAACC,SAAS,CAAC4B,IAAI,EAAE,IAAI,EAAE,CAAC,CAAC,CAAC;QACjFU,SAAS,GAAG,IAAIzB,KAAK,CAAC,sCAAsC0B,KAAK,EAAE,CAAC;QACpE,SAAS,CAAC;MACZ;IACF,CAAC,CAAC,OAAO5B,KAAK,EAAE;MACd7B,OAAO,CAAC6B,KAAK,CAAC,oBAAoB4B,KAAK,GAAG,EAAE5B,KAAK,CAAC;MAClD2B,SAAS,GAAG3B,KAAK;MACjB;IACF;EACF;;EAEA;EACA,MAAM2B,SAAS,IAAI,IAAIzB,KAAK,CAAC,4BAA4B,CAAC;AAC5D","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}